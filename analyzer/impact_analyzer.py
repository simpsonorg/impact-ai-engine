import json
import os
import re
from openai import OpenAI
from analyzer.pgvector_rag import search_similar_chunks

# OpenAI Client
try:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except:
    client = None


def build_llm_prompt(pr_title, changed_files, semantic_graph):
    """
    SAME dashboard formatting as before.
    Uses semantic_graph instead of static dependency graph.
    """

    return f"""
You are a senior software architect writing a beautifully structured, highly readable
GitHub Pull Request impact analysis comment.

Use MARKDOWN ONLY. GitHub must render it cleanly.

Generate a detailed report with the following structure:

================================================================================
# üéõÔ∏è Impact Analysis Dashboard (Top Summary)
- Use a **Markdown table** summarizing:
  - Severity (LOW / MEDIUM / HIGH with emoji)
  - Impacted services
  - Count of changed files
  - High-level recommendation

================================================================================
# üìù Summary of What Changed
- Explain PR Title impact in 3‚Äì5 sentences.
- Describe how the changed files influence upstream/downstream contracts.

================================================================================
# üß© Downstream Impact Breakdown

For EACH impacted service in the dependency graph:
## <service-name>
**Why impacted:** 1‚Äì2 sentences  
**Files to review:** List **actual files**  
**Recommended fixes:** Bullet points  
**Risk:** LOW / MEDIUM / HIGH  

================================================================================
# üß™ Recommended Test Coverage
- List 4‚Äì7 test cases developers should run.

================================================================================
# üß† Final Reviewer Guidance
- Summarize in 3‚Äì6 sentences what the reviewer should consider before merging.
------------------------------------------------------------------------------

### INPUT DATA

PR Title:
{pr_title}

Changed Files:
{json.dumps(changed_files, indent=2)}

Semantic Dependency Graph (pgvector-based):
{json.dumps(semantic_graph, indent=2)}

Relevant RAG Chunks (Top semantic matches):
{json.dumps(semantic_graph.get("rag_chunks", []), indent=2)}

------------------------------------------------------------------------------
MUST be pure Markdown ‚Äî no JSON, no backticks around the entire output.
"""


def analyze(pr_title, changed_files, semantic_graph):
    if not client:
        return "‚ö†Ô∏è Missing OPENAI_API_KEY for analysis."

    prompt = build_llm_prompt(pr_title, changed_files, semantic_graph)

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1900,
            temperature=0.2,
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"‚ö†Ô∏è AI engine failed:\n```\n{str(e)}\n```"
